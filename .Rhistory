str(slm$summary.random)
str(slm$summary.fixed)
#beta: from 507-520 in the random effects
slm$summary.random$idx[(dim(mmatrix)[1]) + 1: dim(mmatrix)[2], ]
View(boston.tr)
str(slm$marginals.hyperpar) #marginal posterior distribution p(rho| sigma2, beta, X,Y...)
x11()
par(mfrow=c(1,2))
plot(slm$marginals.hyperpar[[1]], type = "l", xlab = "1/sigma^2", ylab = "density")
plot(slm$marginals.hyperpar[[2]], type = "l", xlab = "rho", ylab = "density",
xlim = c(0.68, 0.92), ylim = c(0,30))
x11()
par(mfrow=c(1,2))
plot(slm$marginals.hyperpar[[1]], type = "l", xlab = "1/sigma^2", ylab = "density")
plot(slm$marginals.hyperpar[[2]], type = "l", xlab = "rho", ylab = "density",
xlim = c(0.68, 0.92), ylim = c(0,30))
str(slm$marginals.random)
slm$marginals.random[[1]][[1]]
slm$marginals.random[[1]][[507]]
x11()
par(mfrow=c(1,2))
plot(slm$marginals.random[[1]][[507]], type = "l", xlab = "1/sigma^2", ylab = "density")
plot(slm$marginals.random[[1]][[508]], type = "l", xlab = "rho", ylab = "density",
xlim = c(0.68, 0.92), ylim = c(0,30))
mmatrix
mmatrix2 <- cbind(mmatrix, create_WX(mmatrix, lw, prefix = "lag"))
mmatrix2[1:10,]
Q.beta.lag<- Diagonal(n = dim(mmatrix2)[2], x = 0.0001)
sdm<-inla( log(CMEDV) ~ -1 +
f(idx, model="slm",
args.slm=list(rho.min = rho.min,
rho.max = rho.max,
W=W,
X=mmatrix2,  #include de WX
Q.beta=Q.beta.lag),
hyper=hyper.slm),
data=datafr,
family="gaussian",
control.family = list(hyper=zero.variance),
control.compute=list(dic=TRUE, cpo=TRUE,config = TRUE)
)
tab.x<-sdm$summary.random$idx[dim(mmatrix2)[1] + 1: dim(mmatrix)[2], ]
tab.x
tab.x.lag<-sdm$summary.random$idx[dim(mmatrix2)[1] + dim(mmatrix)[2]+1:dim(mmatrix)[2], ]
m<- dim(tab.x.lag)[1]
nm<-colnames(mmatrix)
tab.x.lag<-tab.x.lag[-m,] #remove the 'lagged intercept'
rownames(tab.x)<-colnames(mmatrix)
tab.x[,1:3]
rownames(tab.x.lag)<-paste(colnames(mmatrix)[2:m],'.lag')
tab.x.lag[,1:3]
summary(slm)
?inla.posterior.sample
samp_slm <- inla.posterior.sample(10000, slm)
length(lengths(samp_slm)) #we have a list of 10000 lists
lengths(samp_slm[[1]]) #each list contains the sample for the hyperparameters and for the latent effects
str(samp_slm[[1]][[1]])
str(samp_slm[[1]][[2]])
samp_slm[[1]][[2]][1:10,]
str(samp_slm[[1]][[3]]) #and the marginal densities
#it is not trivial to obtain the chain, in order to plot the credible intervals it is more
#useful to use the quantiles for each parameters
sumy<-slm$summary.random$idx[(dim(mmatrix)[1]) + 1: dim(mmatrix)[2], ]
rownames(sumy)<-colnames(mmatrix)
sumy
Q<-matrix(nrow=p,ncol=3)
rownames(Q)<-colnames(mmatrix)
colnames(Q)<-c('lower','m','upper')
#example: IC for regressors
for(i in 1:p){
Q[i,1]<-inla.qmarginal(0.05, slm$marginals.random$idx[[(dim(mmatrix)[1]) + i]])
Q[i,2]<-inla.qmarginal(0.5, slm$marginals.random$idx[[(dim(mmatrix)[1]) + i]])
Q[i,3]<-inla.qmarginal(0.95, slm$marginals.random$idx[[(dim(mmatrix)[1]) + i]])
}
Q
require(plotrix)
x11()
plotCI(1:p,Q[,2], ui=Q[,3], li=Q[,1],xlab='',ylab='',main='credible intervals (10%)')
abline(h=0, lty=2,col='red')
require(plotrix)
x11()
plotCI(1:p,Q[,2], ui=Q[,3], li=Q[,1],xlab='',ylab='',main='credible intervals (10%)')
abline(h=0, lty=2,col='red')
intall.packages(plotrix)
intall.packages('plotrix')
intall.package('plotrix')
install.package('plotrix')
install.packages('plotrix')
require(plotrix)
x11()
plotCI(1:p,Q[,2], ui=Q[,3], li=Q[,1],xlab='',ylab='',main='credible intervals (10%)')
abline(h=0, lty=2,col='red')
colnames(mmatrix)[c(1,6,9,10,14)]
inla.qmarginal(0.05,slm$marginals.hyperpar[[2]])
inla.qmarginal(0.05,slm$marginals.hyperpar[[2]])
inla.qmarginal(0.95,slm$marginals.hyperpar[[2]])
#SEM model
sem_rw <- inla(log(CMEDV) ~ CRIM + ZN + INDUS + CHAS +
f(NOX, model = "rw2", scale.model = TRUE, hyper = list(prec = list(param = c(2000 * 2000 / 10, 2000 / 10))))+
I(RM^2) +
AGE + log(DIS) + log(RAD) + TAX + PTRATIO + B + log(LSTAT)+
f(idx, model = "slm",
args.slm = list(rho.min = rho.min,
rho.max = rho.max,
W=W,
X=mmatrix,
Q.beta=Q.beta),
hyper = hyper.slm),
data = datafr,
family = "gaussian",
control.family = list(hyper = zero.variance),
control.compute = list(dic = TRUE, cpo = TRUE)
)
#we still have the fixed effets (the coefficients of the formula)
sem_rw$summary.fixed
#we still have the fixed effets (the coefficients of the formula)
summary(sem_rw)
#but in the random effecta now we find also NOX
sem_rw$summary.random
lengths(sem_rw$summary.random)
str(sem_rw$summary.random)
x11()
plot(sem_rw$summary.random$NOX[, 1:2], type = "l", lwd = 2,
ylim = c(-0.4, 0.2),
main = "SEM model", xlab = "NOX", ylab = "f(NOX)")
lines(sem_rw$summary.random$NOX[, c(1, 4)], lty = 2)
lines(sem_rw$summary.random$NOX[, c(1, 6)], lty = 2)
#here it the shapefile for boston dataset
boston.tr <- st_read(system.file("shapes/boston_tracts.shp", package="spData")[1])
str(boston.tr)
boston.tr$SEM<-sem$summary.random$idx[1:506,2]
boston.tr$SLM<-slm$summary.random$idx[1:506,2]
str(boston.tr)
library(tmap)
install.packages('tmap')
library(tmap)
x11()
tm_shape(boston.tr) +
tm_fill(c("SEM",'SLM'), style="fisher", n=8, midpoint=0, title="Random effects") +
tm_facets(free.scales=FALSE)  +
tm_layout(panel.labels=c("SEM", "SLM"), bg='grey95')
library(ggplot2)
ggplot(boston.c, aes(LON, LAT, col=SEM)) +
geom_point( size=2) +
labs(title="Boston house",x="",y="")
ggplot(data2, aes(LON, LAT, col=SEM)) +
geom_point( size=2) +
labs(title="Boston house",x="",y="")
data2<- boston.c
data2$SEM<-sem$summary.random$idx[1:506,2]
data2$SLM<-slm$summary.random$idx[1:506,2]
ggplot(data2, aes(LON, LAT, col=SEM)) +
geom_point( size=2) +
labs(title="Boston house",x="",y="")
ggplot(data2, aes(LON, LAT, col=SEM)) +
geom_point( size=2) +
labs(title="Boston house",x="",y="")
# Some functions to compute fitted values and impacts from inla models
#   fitted using the slm latent effect
# Function to re-estimate the fitted values using sampling
#
#W: adjacency amtrix (as Matrix)
fun_fitted <- function(..., W) {
n.areas <- nrow(W)
coeffs <- idx[-c(1:n.areas)]
rho <- rho.min + theta[2] * (rho.max - rho.min)
# Add white noise
XBe <- (mmatrix %*% matrix(coeffs, ncol = 1)) +
matrix(rnorm(n.areas, 0, exp(- theta[1] / 2)), ncol = 1)
res <- as.vector(solve(Diagonal(n.areas, x = 1) - rho * W, XBe))
return(res)
}
# Compute impacts using sampling
#
#n.areas: Number of areas
#e.values: Eigenvalues of the adjacency matrix (as Matrix) to compute impacts
#n.var: Variable to compute the impats (0 = Intercept, etc.)
#intercept: Has the model an intercept?
#lag: Whether lagged variables are included; in the linear predictor: covariates, lagged covariates (in the same order)
compute_impacts_slm <- function(..., n.areas, e.values,
n.var, intercept = TRUE, lag = FALSE) {
# Number of covariates + lagged covariates; used for lagged covariates
n.cov <- length(idx) - n.areas - intercept #-1 is the intercept
# Coefficient
coeff <- idx[n.areas + intercept + n.var]
# Spat. autocorr. coefficient
rho <- rho.min + theta[2] * (rho.max - rho.min)
# Impacts
total.impact <- coeff / (1 - rho)
dir.impact <- sum(1 / (1 - rho * e.values)) * coeff / n.areas
# If lagged covariates
if(lag) {
# VAlue of the coefficient of the lagged covariate
lag.coeff <- idx[n.areas + intercept + n.cov / 2  + n.var]
# Compute trace of (I - rho * W)^{-1} W using a Taylor expansion of order 10
rho.e.values <- e.values
trIrhoWW <- sum(e.values)
for(i in 1:10) {
rho.e.values <- rho * e.values * rho.e.values
trIrhoWW  <- trIrhoWW  + sum(rho.e.values)
}
# Update impacts
total.impact <- total.impact + lag.coeff / (1 - rho)
dir.impact <- dir.impact + trIrhoWW * lag.coeff / n.areas
}
# Indirect impacts
indir.impact <- total.impact - dir.impact
return(c(dir.impact, indir.impact, total.impact))
}
# Compute impacts for first covariate
compute_impacts_sem_probit <- function(..., n.areas,
n.var, intercept = TRUE, lag = FALSE, mmatrix) {
# Number of covariates + lagged covariates; used for lagged covariates
#n.cov <- length(idx) - n.areas - intercept #-1 is the intercept
n.cov <- ncol(mmatrix) - intercept
# Coefficient
coeff <- eval(parse(text = colnames(mmatrix)[intercept + n.var]))
# Covariate
#X <- mmatrix[, intercept + n.var]
eta <- Predictor[1:n.areas]
# Spat. autocorr. coefficient (NOT USED for SEM)
#rho <- rho.min + theta[2] * (rho.max - rho.min)
if(!lag) {
# Impacts
#eta <- X * coeff
total.impact <- mean(dnorm(eta)) * coeff
dir.impact <- total.impact
#dir.impact <- sum(1 / (1 - rho * e.values)) * coeff / n.areas
} else { # If lagged covariates
# Value of the coefficient of the lagged covariate
coeff_lag <- eval(parse(text = colnames(mmatrix)[intercept + n.cov / 2 + n.var]))
#X_lag <- mmatrix[, intercept + n.cov / 2 + n.var]
# 'linear pedictor' eta
#eta <- X * coeff + X_lag * coeff_lag
# Impacts
dir.impact <- mean(dnorm(eta)) * coeff
total.impact <- mean(dnorm(eta)) * (coeff + coeff_lag)
}
# Indirect impacts
indir.impact <- total.impact - dir.impact
return(c(dir.impact, indir.impact, total.impact))
}
# Compute all impacts
# inla_samples: Samples from inla.posterior.sample
# n.var: NOW the number of covariates to compute the impacts
compute_impacts_sem_probit_all <- function(inla_samples, n.areas,
n.var, intercept = TRUE, lag = FALSE, mmatrix) {
res <- data.frame(sapply(1:n.var, function(X) {
impacts <- inla.posterior.sample.eval(compute_impacts_sem_probit,
inla_samples, n.areas = n.areas, n.var = X, lag = lag,
mmatrix = mmatrix)
impacts <- c(apply(impacts, 1, mean), apply(impacts, 1, sd))
return(impacts)
}))
colnames(res) <- colnames(mmatrix)[intercept + 1:n.var]
rownames(res) <- c("direct (mean)", "indirect (mean)", "total (mean)",
"direct (s.d.)", "indirect (s.d.)", "total (s.d.)")
return(res)
}
# Compute impacts for first covariate
compute_impacts_slm_probit <- function(..., n.areas, W,
n.var, intercept = TRUE, lag = FALSE, mmatrix) {
# Number of covariates + lagged covariates; used for lagged covariates
#n.cov <- length(idx) - n.areas - intercept #-1 is the intercept
n.cov <- ncol(mmatrix) - intercept
# Coefficient
coeff <- idx[n.areas + intercept + n.var]
# Covariate
#X <- mmatrix[, intercept + n.var]
eta <- Predictor[1:n.areas]
deta <- dnorm(eta)
# Spat. autocorr. coefficient (NOT USED for SEM)
rho <- rho.min + theta[1] * (rho.max - rho.min)
# Traces of d(f(eta)) %*% W^k, k=0, 1, ..., 11
# Note that traces are already divided by n.areas
DETA <- Diagonal(n.areas, x = deta)
TRACES <- rep(NA, 11 + lag)
TRACES[1] <- mean(diag(DETA))
TRACES[2] <- 0
AUX <- DETA %*% W
for(i in 3:(11 + lag)) {
AUX <- AUX %*% W
TRACES[i] <- mean(diag(AUX))
}
if(!lag) {
# Impacts
#eta <- X * coeff
total.impact <- mean(deta) * coeff / (1 - rho)
# Direct impacts
dir.impact <- coeff * sum((rho^(0:10)) * TRACES[1:11])
} else { # If lagged covariates
# Value of the coefficient of the lagged covariate
coeff_lag <- idx[n.areas + intercept + n.cov / 2  + n.var]
#X_lag <- mmatrix[, intercept + n.cov / 2 + n.var]
# 'linear pedictor' eta
#eta <- X * coeff + X_lag * coeff_lag
# Impacts
total.impact <- mean(deta) * (coeff + coeff_lag) / (1 - rho)
# Direct impacts
dir.impact <- coeff * sum((rho^(0:10)) * TRACES[1:11])
dir.impact <- dir.impact +
coeff_lag * sum((rho^(0:10) * TRACES[1 + 1:11]))
}
# Indirect impacts
indir.impact <- total.impact - dir.impact
return(c(dir.impact, indir.impact, total.impact))
}
# Compute all impacts
# inla_samples: Samples from inla.posterior.sample
# n.var: NOW the number of covariates to compute the impacts
compute_impacts_slm_probit_all <- function(inla_samples, n.areas,
W, n.var, intercept = TRUE, lag = FALSE, mmatrix) {
res <- data.frame(sapply(1:n.var, function(X) {
impacts <- inla.posterior.sample.eval(compute_impacts_slm_probit,
inla_samples, n.areas = n.areas, W = W, n.var = X, lag = lag,
mmatrix = mmatrix)
impacts <- c(apply(impacts, 1, mean), apply(impacts, 1, sd))
return(impacts)
}))
colnames(res) <- colnames(mmatrix)[intercept + 1:n.var]
rownames(res) <- c("direct (mean)", "indirect (mean)", "total (mean)",
"direct (s.d.)", "indirect (s.d.)", "total (s.d.)")
return(res)
}
install.packages('CARBayes')
install.packages('CARBayesdata')
install.packages('GGally')
install.packages('coda')
# These IGs are small areas that have a median population of 4239
library(CARBayes)
library(CARBayesdata)
library(sp)
library(leaflet)
library(spdep)
#this is the shp
?GGHB.IG
data(GGHB.IG)
str(GGHB.IG,max.level = 2)
str(GGHB.IG@data)
?pricedata
data(pricedata)
datafr<-pricedata
str(datafr)
data(pricedata)
datafr<-pricedata
str(datafr)
datafr$logprice <- log(datafr$price)
#summary of the desctiption of data
library(GGally)
install.packages('ggplot2')
install.packages("ggplot2")
# These IGs are small areas that have a median population of 4239
library(CARBayes)
library(CARBayesdata)
library(sp)
library(leaflet)
library(spdep)
#this is the shp
?GGHB.IG
data(GGHB.IG)
str(GGHB.IG,max.level = 2)
str(GGHB.IG@data)
?pricedata
data(pricedata)
datafr<-pricedata
str(datafr)
datafr$logprice <- log(datafr$price)
#summary of the desctiption of data
library(GGally)
x11()
ggpairs(data = datafr, columns = c(8, 3:7))
pricedata.sp <- merge(x=GGHB.IG, y=datafr, by="IG", all.x=FALSE)
str(pricedata.sp@data)
#we need to transform the coordinates in order to print on leaflet
library(rgdal)
pricedata.sp <- spTransform(pricedata.sp , CRS("+proj=longlat +datum=WGS84 +no_defs"))
#the domain are the possible values of the variable of interest
colours <- colorNumeric(palette = "YlOrRd", domain = pricedata.sp@data$price)
leaflet(data=pricedata.sp) %>%
addTiles() %>%
addPolygons(fillColor = ~colours(price), color="", weight=1,fillOpacity = 0.7) %>%
addLegend(pal = colours, values = pricedata.sp@data$price, opacity = 1,title="Price") %>%
addScaleBar(position="bottomleft")
leaflet(data=pricedata.sp) %>%
addTiles() %>%
addPolygons(fillColor = ~colours(price), color="", weight=1,fillOpacity = 0.7) %>%
addLegend(pal = colours, values = pricedata.sp@data$price, opacity = 1,title="Price") %>%
addScaleBar(position="bottomleft")
leaflet(data=pricedata.sp) %>%
addTiles()
leaflet(data=pricedata.sp) %>%
addTiles() %>%
addPolygons(fillColor = ~colours(price), color="", weight=1,fillOpacity = 0.7) %>%
addLegend(pal = colours, values = pricedata.sp@data$price, opacity = 1,title="Price") %>%
addScaleBar(position="bottomleft")
#define W
W.nb <- poly2nb(pricedata.sp, row.names = rownames(pricedata.sp@data))
W.list <- nb2listw(W.nb, style="B") #NB. the style is Binary
W <- nb2mat(W.nb,style='B')
mod.lm <- lm(logprice~crime+rooms+sales+factor(type) + driveshop, data=datafr)
summary(mod.lm)
lm.morantest(mod.lm, listw=W.list)
?S.CARbym
?CARleroux
?S.CARleroux
mod.car <- S.CARleroux(logprice~crime+rooms+sales+factor(type) + driveshop,data=datafr,
family="gaussian", W=W,
burnin=100000, n.sample=300000, thin=100)
# my chain will contain (300000-100000)/100 iterations
str(mod.car)
mod.car$summary.results
chains <- mod.car$samples
str(chains)
summary(mod.car)
print(mod.car)
print(mod.car)
#----------------------------------------------
# check of convergence for the chains:
#-----------------------------------------------
library(coda)
beta.samples <- mcmc.list(chains$beta)
chains$bet
str(beta.samples)
dim(beta.samples[[1]])
p <- 3 # regressors
nam<-c('crime','rooms','sales')
x11()
par(mfrow=c(3,p))
for(i in 1+1:p)  {
plot( as.vector(beta.samples[[1]][,i]),type='l',xlab='',ylab='',main=nam[i-1])
abline(h=mean(beta.samples[[1]][,i]),col='red')}
for(i in 1+1:p)  acf( as.vector(beta.samples[[1]][,i]),lwd=3,col="red3",xlab='',main='')
for(i in 1+1:p)  geweke.plot(beta.samples[ ,i],auto.layout =F)
x11()
par(mfrow=c(3,1))
plot( as.vector(rho.samples[[1]]),type='l',xlab='',ylab='',main='rho')
abline(h=mean(rho.samples[[1]]),col='red')
acf( as.vector(rho.samples[[1]]),lwd=3,col="red3",xlab='',main='')
geweke.plot(rho.samples,auto.layout =F)
x11()
par(mfrow=c(3,1))
plot( as.vector(rho.samples[[1]]),type='l',xlab='',ylab='',main='rho')
abline(h=mean(rho.samples[[1]]),col='red')
acf( as.vector(rho.samples[[1]]),lwd=3,col="red3",xlab='',main='')
geweke.plot(rho.samples,auto.layout =F)
?effectiveSize
effectiveSize(beta.samples)
?respiratorydata
data(respiratorydata)
head(respiratorydata)
respiratorydata.sp <- merge(x=GGHB.IG, y=respiratorydata, by="IG", all.x=FALSE)
respiratorydata.sp <- spTransform(respiratorydata.sp,CRS("+proj=longlat +datum=WGS84 +no_defs"))
colours <- colorNumeric(palette = "YlOrRd", domain = respiratorydata.sp@data$SMR)
leaflet(data=respiratorydata.sp) %>%
addTiles() %>%
addPolygons(fillColor = ~colours(SMR), color="", weight=1, fillOpacity = 0.7) %>%
addLegend(pal = colours, values = respiratorydata.sp@data$SMR, opacity = 1, title="SMR") %>%
addScaleBar(position="bottomleft")
W.nb <- poly2nb(respiratorydata.sp, row.names = rownames(respiratorydata.sp@data))
W <- nb2mat(W.nb, style="B")
income <- respiratorydata.sp@data$incomedep
Z.incomedep <- as.matrix(dist(income, diag=TRUE, upper=TRUE))
Z.incomedep
str(z.incomedep)
str(Z.incomedep)
Z.incomedep[1:10]
tmp <- W*Z.incomedep
tmp
#theta is the risk
car.diss <- S.CARdissimilarity(observed ~ offset(log(expected)),
data=respiratorydata.sp@data,
family="poisson",
W=W, Z=list(Z.incomedep=Z.incomedep),                                W.binary=TRUE,
burnin=100000, n.sample=300000, thin=100)
save(car.diss, file='car_dissimilarity.RData')
#save(car.diss, file='car_dissimilarity.RData')
load('car_dissimilarity.RData')
#theta is the risk
car.diss <- S.CARdissimilarity(observed ~ offset(log(expected)),
data=respiratorydata.sp@data,
family="poisson",
W=W, Z=list(Z.incomedep=Z.incomedep),                                W.binary=TRUE,
burnin=100000, n.sample=300000, thin=100)
save(car.diss, file='car_dissimilarity.RData')
#load('car_dissimilarity.RData')
print(car.diss)
#alpha: is the threshold value for the regression parameter ??, below which the dissimilarity metric has
#no effect in identifying boundaries in the response (random effects) surface
summary(car.diss)
#Values equal to zero represent a boundary, values
#equal to one correspond to no boundary, while NA values correspond to non-adjacent areas.
W.boundary <- car.diss$localised.structure$W.posterior
#e creation of the boundaries as a SpatialPoints
border.locations <- car.diss$localised.structure$W.posterior
respiratorydata.sp@data$risk <- car.diss$fitted.values /  respiratorydata.sp@data$expected
#formats boundary to enable plotting
boundary.final <- highlight.borders(border.locations=border.locations, spdata=respiratorydata.sp)
setwd("D:/MisDocumentos/tesis_codigos/COVID")
library(digest)
library(rgeoda)
library(sf)
guerry_path <- system.file("extdata", "Guerry.shp", package = "rgeoda")
guerry <- st_read(guerry_path)
w_knn6 <- knn_weights(guerry, 6)
#data <- covid[c('mes2','mes3','mes4','mes5','mes6','mes7','mes8','mes9','mes10','mes11','mes12','mes13','mes14','mes15','mes16')]
data <- guerry[c('Crm_prs','Crm_prp','Litercy','Donatns','Infants','Suicids')]
bound_vals <- guerry['Pop1831']
maxp_guerry <- maxp_tabu(w_knn6,data,bound_variable = bound_vals, min_bound = 3236.67)
maxp_guerry$Clusters
guerry['maxp'] <- maxp_guerry$Clusters
direc <- paste('df_R/','guerry','.shp',sep = '')
st_write(guerry,direc, delete_layer = T)
